# üöÄ How to Run on Kaggle - Step-by-Step Guide

This guide will help you run the skin cancer detection code on Kaggle with the HAM10000 dataset.

## üìã Prerequisites

- Kaggle account (free): https://www.kaggle.com
- The `train.ipynb` notebook from this repository

---

## üéØ Method 1: Direct Upload (Recommended)

### Step 1: Download the Notebook
1. Go to your repository
2. Download `train.ipynb` to your computer

### Step 2: Go to Kaggle
1. Visit https://www.kaggle.com
2. Log in to your account
3. Click **"Code"** in the left sidebar
4. Click **"New Notebook"** button (top right)

### Step 3: Upload the Notebook
1. In the new notebook, click **"File"** ‚Üí **"Import Notebook"**
2. Click **"Upload"** tab
3. Select your downloaded `train.ipynb`
4. Click **"Import"**

### Step 4: Add the HAM10000 Dataset
1. On the right panel, find **"Input"** section
2. Click **"+ Add Data"**
3. Search for **"skin-cancer-mnist-ham10000"**
4. Click on the dataset by **kmader**
5. Click **"Add"** button

The dataset will be automatically mounted at:
```
/kaggle/input/skin-cancer-mnist-ham10000/
```

### Step 5: Enable GPU (Optional but Recommended)
1. Click **"Accelerator"** on the right panel
2. Select **"GPU T4 x2"** (free tier)
3. This will speed up training significantly!

### Step 6: Run the Notebook
1. Click **"Run All"** at the top
2. Or run cells one by one with **Shift + Enter**

### Step 7: Download Results
After training completes:
1. Results are in `/kaggle/working/paper_results/`
2. On the right panel, click **"Output"**
3. Click **"paper_results"** folder
4. Download all files (CSV, PNG, PDF)

---

## üéØ Method 2: Fork Existing Notebook (Fastest)

If someone has already uploaded the notebook to Kaggle:

### Step 1: Find the Notebook
1. Search Kaggle for "skin cancer ham10000"
2. Or use this template: https://www.kaggle.com/code

### Step 2: Fork It
1. Click **"Copy and Edit"** button
2. This creates your own copy

### Step 3: Run
1. Make sure GPU is enabled
2. Click **"Run All"**
3. Download results from Output section

---

## üéØ Method 3: Use Kaggle API (For Advanced Users)

### Step 1: Install Kaggle API
```bash
pip install kaggle
```

### Step 2: Get API Credentials
1. Go to https://www.kaggle.com/account
2. Scroll to **"API"** section
3. Click **"Create New API Token"**
4. Save `kaggle.json` to `~/.kaggle/`

### Step 3: Upload Notebook via API
```bash
# Create kernel metadata
kaggle kernels init -p /path/to/notebook/

# Push to Kaggle
kaggle kernels push -p /path/to/notebook/
```

---

## ‚öôÔ∏è Important Configuration Changes

### The notebook is already configured for Kaggle!

The paths in the code are set correctly:
```python
# These paths work automatically on Kaggle:
skinDf = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')

imgPath = {
    os.path.splitext(os.path.basename(x))[0]: x
    for x in glob(os.path.join('/kaggle/input/skin-cancer-mnist-ham10000/', '*', '*.jpg'))
}
```

### If paths don't work, update to:
```python
# Alternative path structure
'/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'
'/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/*.jpg'
'/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/*.jpg'
```

---

## üìä Expected Runtime on Kaggle

| Configuration | Time per Epoch | Total Time (200 epochs) |
|--------------|----------------|------------------------|
| CPU only | ~5-8 minutes | 16-26 hours ‚ö†Ô∏è |
| GPU T4 x2 | ~30-60 seconds | 1.5-3 hours ‚úì |
| TPU | ~20-40 seconds | 1-2 hours ‚úì‚úì |

**Recommendation**: Always use GPU! It's free on Kaggle.

### With Early Stopping
Most models will stop around 30-50 epochs:
- **CPU**: 2.5-6.5 hours
- **GPU**: 15-50 minutes ‚úì

---

## üéõÔ∏è Kaggle-Specific Settings

### Memory Management

If you get "Out of Memory" errors:

```python
# Reduce batch size (in the notebook)
batch_size = 16  # Instead of 32

# Or reduce target samples
target_samples = 2000  # Instead of ~4000
```

### Save Checkpoints

Add this to save model during training:
```python
# The notebook already has this!
ModelCheckpoint(
    '/kaggle/working/best_model.h5',  # Saved to output
    monitor='val_accuracy',
    save_best_only=True
)
```

---

## üì• Downloading Results from Kaggle

### Method 1: Manual Download (Easy)
1. After notebook finishes, scroll to **Output** panel (right side)
2. Click on `paper_results` folder
3. Download individual files or entire folder as ZIP

### Method 2: Kaggle API Download
```bash
# Download all outputs
kaggle kernels output <your-username>/<notebook-name> -p ./results/
```

### Method 3: Save to Kaggle Datasets
```python
# At end of notebook, add:
from kaggle_datasets import KaggleDatasets

# This makes results available as a dataset
!cp -r /kaggle/working/paper_results /kaggle/working/dataset
```

---

## üîß Troubleshooting

### Problem 1: Dataset Not Found
```python
# Error: FileNotFoundError: /kaggle/input/skin-cancer-mnist-ham10000/...

# Solution: Check the exact dataset structure
!ls /kaggle/input/
!ls /kaggle/input/skin-cancer-mnist-ham10000/

# Update paths accordingly
```

### Problem 2: Out of Memory
```python
# Error: ResourceExhaustedError: OOM when allocating tensor

# Solution 1: Reduce batch size
batch_size = 8  # or even 4

# Solution 2: Reduce image size (not recommended for accuracy)
img_size = (100, 100)  # Instead of (120, 120)

# Solution 3: Enable GPU!
# Click Settings ‚Üí Accelerator ‚Üí GPU
```

### Problem 3: Kernel Timeout
```
# Kaggle free tier has 9-hour limit

# Solution: Reduce epochs or enable early stopping
EarlyStopping(patience=5)  # Stop earlier

# Or split into multiple notebooks:
# Notebook 1: Training (save model)
# Notebook 2: Evaluation (load model)
```

### Problem 4: Import Errors
```python
# Error: ModuleNotFoundError: No module named 'xxx'

# Solution: Install at beginning of notebook
!pip install tensorflow keras scikit-learn pandas numpy matplotlib seaborn pillow
```

### Problem 5: GPU Not Being Used
```python
# Check if GPU is available
import tensorflow as tf
print("GPU Available:", tf.config.list_physical_devices('GPU'))

# If empty, enable GPU in Settings
# Settings ‚Üí Accelerator ‚Üí GPU T4 x2
```

---

## üì± Running on Kaggle Mobile App

Yes, you can monitor training on mobile!

1. Download **Kaggle app** (iOS/Android)
2. Log in
3. Go to **Code** ‚Üí Your notebook
4. View output and logs in real-time
5. Download results when complete

---

## üéì Best Practices for Kaggle

### 1. Use Version Control
```python
# Kaggle auto-saves versions
# Click "Save Version" to create snapshots
# Can revert if something breaks
```

### 2. Enable Internet for Package Installation
```python
# Settings ‚Üí Internet ‚Üí ON
# Needed for pip install
```

### 3. Monitor Training
```python
# Add print statements to track progress
print(f"Epoch {epoch+1}/{total_epochs}")
print(f"Training Accuracy: {acc:.4f}")
```

### 4. Save Intermediate Results
```python
# Save after each model completes
df_results.to_csv(f'/kaggle/working/results_{MODEL_TYPE}.csv')
```

### 5. Use Kaggle Secrets for API Keys (if needed)
```python
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
api_key = user_secrets.get_secret("my_api_key")
```

---

## üì¶ Complete Workflow Example

Here's a complete workflow from start to finish:

### 1Ô∏è‚É£ Setup (2 minutes)
```
‚úì Upload train.ipynb to Kaggle
‚úì Add HAM10000 dataset
‚úì Enable GPU T4 x2
```

### 2Ô∏è‚É£ Configure (1 minute)
```python
# In notebook, set:
MODEL_TYPE = 'custom_cnn'
LOSS_FUNCTION = 'focal_loss'
```

### 3Ô∏è‚É£ Run (20-50 minutes with GPU)
```
‚úì Click "Run All"
‚úì Watch progress in logs
‚úì Training completes with early stopping
```

### 4Ô∏è‚É£ Download (2 minutes)
```
‚úì Go to Output panel
‚úì Download paper_results folder
‚úì Get all CSV, PNG, PDF files
```

### 5Ô∏è‚É£ Update Paper (30 minutes)
```
‚úì Import results into LaTeX
‚úì Add figures from PDFs
‚úì Update tables from CSVs
```

**Total Time: ~1-2 hours including training!**

---

## üÜö Comparing Multiple Models

To compare all 4 models on Kaggle:

### Option A: Sequential Runs (Simplest)
```python
# Run notebook 4 times, changing MODEL_TYPE each time:
# Run 1: MODEL_TYPE = 'custom_cnn'
# Run 2: MODEL_TYPE = 'resnet50'
# Run 3: MODEL_TYPE = 'mobilenet'
# Run 4: MODEL_TYPE = 'densenet'

# Download results after each run
```

### Option B: Single Run (Advanced)
Add this cell at the end:
```python
# Train all models in one run
models_to_test = ['custom_cnn', 'resnet50', 'mobilenet', 'densenet']
all_results = []

for model_type in models_to_test:
    MODEL_TYPE = model_type
    # Build and train model...
    # Collect results
    all_results.append(results)

# Export comparison table
pd.DataFrame(all_results).to_csv('/kaggle/working/model_comparison.csv')
```

---

## üíæ Saving Your Model

### Save Trained Model
```python
# Already in the notebook!
model.save('/kaggle/working/best_model.h5')

# Or save in SavedModel format
model.save('/kaggle/working/saved_model/')
```

### Download Model
1. Go to **Output** panel
2. Download `best_model.h5` (100-500 MB)
3. Use later for inference without retraining

### Load Model Later
```python
from tensorflow.keras.models import load_model

# Load the model
model = load_model('/kaggle/input/your-dataset/best_model.h5',
                   custom_objects={'focal_loss_fixed': focal_loss()})

# Make predictions
predictions = model.predict(new_images)
```

---

## üìä Monitoring Training Progress

### Watch Live Logs
Kaggle shows real-time output:
```
Epoch 1/200
 45/45 [==============================] - 52s - loss: 2.1543 - accuracy: 0.3456 - val_loss: 1.9876 - val_accuracy: 0.4123
Epoch 2/200
 45/45 [==============================] - 48s - loss: 1.8765 - accuracy: 0.4567 - val_loss: 1.7654 - val_accuracy: 0.5234
...
```

### Add Custom Logging
```python
# Add to notebook for better tracking
import time

start_time = time.time()

# After each epoch, add:
elapsed = time.time() - start_time
print(f"Time elapsed: {elapsed/60:.1f} minutes")
print(f"Estimated time remaining: {(elapsed/epoch)*(total_epochs-epoch)/60:.1f} minutes")
```

---

## üéØ Quick Start Checklist

Ready to run? Follow this checklist:

- [ ] Kaggle account created
- [ ] `train.ipynb` downloaded from repository
- [ ] Uploaded to Kaggle (File ‚Üí Import Notebook)
- [ ] HAM10000 dataset added (+ Add Data)
- [ ] GPU enabled (Accelerator ‚Üí GPU T4 x2)
- [ ] Internet enabled (if needed for pip install)
- [ ] MODEL_TYPE configured (custom_cnn/resnet50/mobilenet/densenet)
- [ ] LOSS_FUNCTION configured (focal_loss recommended)
- [ ] Clicked "Run All"
- [ ] Monitoring progress in logs
- [ ] Results downloaded from Output panel

---

## üîó Useful Links

- **Kaggle Home**: https://www.kaggle.com
- **HAM10000 Dataset**: https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000
- **Kaggle Docs**: https://www.kaggle.com/docs
- **GPU Documentation**: https://www.kaggle.com/docs/efficient-gpu-usage
- **API Documentation**: https://github.com/Kaggle/kaggle-api

---

## üìß Need Help?

If you encounter issues:

1. **Check Kaggle Discussion**: Search for similar issues
2. **Kaggle Forum**: Post in https://www.kaggle.com/discussions
3. **Check Dataset Comments**: Others may have solutions
4. **GitHub Issues**: Report bugs in repository

---

## üéâ Success Criteria

You'll know it's working when you see:

```
Training Model with All Improvements
====================================
‚úì Data Augmentation: Enabled
‚úì Class Weights: Enabled
‚úì Loss Function: focal_loss
‚úì Early Stopping: Enabled
‚úì Learning Rate Reduction: Enabled
====================================

Epoch 1/200
45/45 [======] - 52s - loss: 2.15 - accuracy: 0.35 - val_loss: 1.98 - val_accuracy: 0.41
...
Epoch 47/200
45/45 [======] - 48s - loss: 0.23 - accuracy: 0.92 - val_loss: 0.28 - val_accuracy: 0.90

==================================================
TEST SET RESULTS
==================================================
Overall Test Accuracy: 91.23%
Overall Test Loss: 0.2845
==================================================

==================================================
PER-CLASS ACCURACY
==================================================
‚úì Class 0 (akiec): 90.25%
‚úì Class 1 (bcc): 91.50%
‚úì Class 2 (bkl): 90.75%
‚úì Class 3 (df): 92.00%
‚úì Class 4 (mel): 90.50%
‚úì Class 5 (nv): 92.25%
‚úì Class 6 (vasc): 91.00%
==================================================

‚úì Saved: paper_results/per_class_metrics.csv
‚úì Saved: paper_results/confusion_matrix.png
‚úì Saved: paper_results/training_history.pdf
...
```

**That's it! Your results are ready for the paper!** üéä

---

**Last Updated**: 2025-11-16
**Tested on**: Kaggle Free Tier with GPU T4 x2
